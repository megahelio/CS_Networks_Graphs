{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Dataset Network Analysis\n",
    "\n",
    "This notebook analyzes the network of keyword co-occurrences in the collected Bluesky posts dataset. \n",
    "It loads the data, extracts keywords, builds a co-occurrence graph, calculates network metrics, and visualizes the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from itertools import combinations\n",
    "from networkx.algorithms import community\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "INPUT_DIR = \"./data\"\n",
    "OUTPUT_BASE_DIR = \"./analysis_results\"\n",
    "MIN_CO_OCCURRENCES = 1  # Minimum co-occurrences to create an edge\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    filepath = os.path.join(INPUT_DIR, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {len(df)} posts.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network(df, config_name):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING: {config_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # 1. Extract Keywords\n",
    "    if 'keyword' not in df.columns:\n",
    "        print(\"Error: 'keyword' column missing.\")\n",
    "        return\n",
    "        \n",
    "    keywords = df['keyword'].dropna().unique().tolist()\n",
    "    print(f\"Unique Keywords found: {len(keywords)}\")\n",
    "    \n",
    "    # Create specific output directory\n",
    "    out_dir = os.path.join(OUTPUT_BASE_DIR, config_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Compute Co-occurrences\n",
    "    print(\"Computing co-occurrences...\")\n",
    "    co_occurrence_data = []\n",
    "    \n",
    "    # Pre-calculate text presence for speed\n",
    "    # Create a dictionary of sets mapping keyword -> set of indices where it appears\n",
    "    # This is faster than repeated str.contains\n",
    "    keyword_indices = {}\n",
    "    for kw in tqdm(keywords, desc=\"Indexing keywords\"):\n",
    "        # Case-insensitive matching\n",
    "        mask = df['text'].str.contains(kw, case=False, na=False, regex=False)\n",
    "        keyword_indices[kw] = set(df[mask].index)\n",
    "        \n",
    "    for kw1, kw2 in tqdm(combinations(keywords, 2), total=len(keywords)*(len(keywords)-1)//2, desc=\"Calculating Pairs\"):\n",
    "        # Intersection of indices\n",
    "        common_indices = keyword_indices[kw1].intersection(keyword_indices[kw2])\n",
    "        count = len(common_indices)\n",
    "\n",
    "        if count >= MIN_CO_OCCURRENCES:\n",
    "            co_occurrence_data.append({\n",
    "                'w1': kw1,\n",
    "                'w2': kw2,\n",
    "                'count': count\n",
    "            })\n",
    "            \n",
    "    co_df = pd.DataFrame(co_occurrence_data)\n",
    "    print(f\"Found {len(co_df)} pairs with co-occurrences >= {MIN_CO_OCCURRENCES}\")\n",
    "    \n",
    "    if len(co_df) == 0:\n",
    "        print(\"No co-occurrences found. Skipping graph build.\")\n",
    "        return\n",
    "\n",
    "    # 3. Build Graph\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(keywords)\n",
    "    \n",
    "    edges = []\n",
    "    for _, row in co_df.iterrows():\n",
    "        edges.append((row['w1'], row['w2'], {'weight': row['count']}))\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    print(f\"Graph constructed: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    \n",
    "    # 4. Calculate Metrics\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = list(degrees.values())\n",
    "    strength = dict(G.degree(weight='weight'))\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    \n",
    "    if nx.is_connected(G):\n",
    "        closeness = nx.closeness_centrality(G, distance='weight')\n",
    "    else:\n",
    "        # Handle disconnected graph for closeness\n",
    "        closeness = {}\n",
    "        for c in nx.connected_components(G):\n",
    "            subg = G.subgraph(c)\n",
    "            closeness.update(nx.closeness_centrality(subg, distance='weight'))\n",
    "            \n",
    "    clustering = nx.clustering(G)\n",
    "    \n",
    "    # Community Detection\n",
    "    communities = []\n",
    "    modularity = 0\n",
    "    if G.number_of_edges() > 0:\n",
    "        communities = list(community.greedy_modularity_communities(G, weight='weight'))\n",
    "        modularity = community.modularity(G, communities, weight='weight')\n",
    "    \n",
    "    # Save Metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'keyword': keywords,\n",
    "        'degree': [degrees.get(k,0) for k in keywords],\n",
    "        'strength': [strength.get(k,0) for k in keywords],\n",
    "        'betweenness': [betweenness.get(k,0) for k in keywords],\n",
    "        'closeness': [closeness.get(k,0) for k in keywords],\n",
    "        'clustering': [clustering.get(k,0) for k in keywords]\n",
    "    }).sort_values('degree', ascending=False)\n",
    "    \n",
    "    metrics_df.to_csv(os.path.join(out_dir, \"node_metrics.csv\"), index=False)\n",
    "    print(\"Metrics saved to node_metrics.csv\")\n",
    "\n",
    "    # 5. Visualizations\n",
    "    visualize_network(G, degrees, edge_weights=[d['weight'] for u,v,d in G.edges(data=True)], \n",
    "                      communities=communities, out_dir=out_dir, title=config_name)\n",
    "    \n",
    "    return G, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(G, degrees, edge_weights, communities, out_dir, title):\n",
    "    # Node Sizes\n",
    "    node_sizes = [degrees[n] * 100 + 100 for n in G.nodes()]\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "    \n",
    "    # 1. Main Network Plot\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Color map for communities if available\n",
    "    node_colors = 'skyblue'\n",
    "    if communities:\n",
    "        color_map = {}\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "        for i, comm in enumerate(communities):\n",
    "            for node in comm:\n",
    "                color_map[node] = cmap(i % 20)\n",
    "        node_colors = [color_map.get(n, 'lightgray') for n in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9, edgecolors='white')\n",
    "    \n",
    "    # Normalize edge widths\n",
    "    if edge_weights:\n",
    "        max_w = max(edge_weights)\n",
    "        widths = [1 + (w/max_w)*5 for w in edge_weights]\n",
    "        nx.draw_networkx_edges(G, pos, width=widths, alpha=0.4, edge_color='gray')\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"Keyword Network: {title}\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(out_dir, \"network_graph.png\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Degree Distribution Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(list(degrees.values()), bins=20, color='steelblue', edgecolor='black')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.savefig(os.path.join(out_dir, \"degree_dist.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Adjacency Heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    adj_matrix = nx.to_pandas_adjacency(G, weight='weight')\n",
    "    plt.imshow(adj_matrix, cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar(label='Co-occurrence Count')\n",
    "    plt.xticks(range(len(adj_matrix)), adj_matrix.columns, rotation=90)\n",
    "    plt.yticks(range(len(adj_matrix)), adj_matrix.index)\n",
    "    plt.title(\"Adjacency Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"heatmap.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Analysis for all CSV files in data directory\n",
    "count = 0\n",
    "if os.path.exists(INPUT_DIR):\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        if filename.endswith(\".csv\") and \"metrics\" not in filename:\n",
    "            config_name = filename.replace(\".csv\", \"\")\n",
    "            df = load_dataset(filename)\n",
    "            if df is not None:\n",
    "                G, metrics = analyze_network(df, config_name)\n",
    "                count += 1\n",
    "else:\n",
    "    print(f\"Directory {INPUT_DIR} does not exist.\")\n",
    "    \n",
    "if count == 0:\n",
    "    print(\"No CSV files found to analyze.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
